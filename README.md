#  ML From Scratch â€“ furyfist :)

This repository documents my journey in building core ML algorithms **from scratch** using NumPy, and applying them using libraries like `scikit-learn`. Itâ€™s structured for clarity, depth, and real-world understanding.

---

## âœ… Machine Learning Algorithms Tracker

| #  | Algorithm                                             | Theory | NumPy Impl | Sklearn | Project | Notes |
|----|--------------------------------------------------------|--------|------------|---------|---------|-------|
| 1  | Linear Regression                                      | âœ…     | âœ…         | âœ…     | â¬œï¸      |       |
| 2  | Gradient Descent                                       | âœ…     | âœ…         | N/A     | â¬œï¸      | Common optimizer |
| 3  | Logistic Regression                                    | âœ…     | â¬œï¸         | âœ…     | â¬œï¸      | Next after linear |
| 4  | Support Vector Machines (SVM)                          | â¬œï¸     | â¬œï¸         | â¬œï¸      | â¬œï¸      | Hinge loss, kernel trick |
| 5  | Naive Bayes                                            | â¬œï¸     | â¬œï¸         | â¬œï¸      | â¬œï¸      | Probabilistic |
| 6  | K-Nearest Neighbors (KNN)                              | â¬œï¸     | â¬œï¸         | â¬œï¸      | â¬œï¸      | Lazy learning |
| 7  | Decision Trees                                         | â¬œï¸     | â¬œï¸         | â¬œï¸      | â¬œï¸      | Entropy/Gini |
| 8  | Random Forest                                          | â¬œï¸     | â¬œï¸         | â¬œï¸      | â¬œï¸      | Ensemble of trees |
| 9  | Bagging                                                | â¬œï¸     | â¬œï¸         | â¬œï¸      | â¬œï¸      | Bootstrap Aggregation |
| 10 | AdaBoost                                               | â¬œï¸     | â¬œï¸         | â¬œï¸      | â¬œï¸      | Weighted learners |
| 11 | Gradient Boosting                                      | â¬œï¸     | â¬œï¸         | â¬œï¸      | â¬œï¸      | Residual-based |
| 12 | XGBoost                                                | â¬œï¸     | â¬œï¸         | â¬œï¸      | â¬œï¸      | Efficient boosting |
| 13 | Principal Component Analysis (PCA)                     | â¬œï¸     | â¬œï¸         | â¬œï¸      | â¬œï¸      | Dimensionality reduction |
| 14 | K-Means Clustering                                     | â¬œï¸     | â¬œï¸         | â¬œï¸      | â¬œï¸      | Unsupervised |
| 15 | Hierarchical Clustering                                | â¬œï¸     | â¬œï¸         | â¬œï¸      | â¬œï¸      | Dendrograms |
| 16 | DBSCAN                                                 | â¬œï¸     | â¬œï¸         | â¬œï¸      | â¬œï¸      | Density-based |
| 17 | T-SNE (t-Distributed Stochastic Neighbor Embedding)    | â¬œï¸     | â¬œï¸         | â¬œï¸      | â¬œï¸      | For high-dim visualization |

---

## ğŸ§­ Learning Workflow

For each algorithm:
1. âœ… Understand the **theory**: intuition, math, derivations.
2. âœ… Implement using **NumPy**: no ML libraries.
3. âœ… Apply with **scikit-learn** (or suitable lib).
4. âœ… Test on small **projects** or datasets.
5. âœ… Document learnings in Markdown.

## Projects to Make : 
Playlist link : https://www.youtube.com/playlist?list=PLfFghEzKVmjvuSA67LszN1dZ-Dd_pkus6

Project 3: House Price Prediction (Linear Regression, Decision Trees, Random Forest, Gradient Boosting, XGBoost).
Project 1: SONAR Rock vs Mine Prediction (Logistic Regression, KNN, Decision Trees, Random Forest, AdaBoost, Gradient Boosting, XGBoost, SVM).
Project 4: Fake News Prediction (Naive Bayes, Logistic Regression, Decision Trees, Random Forest, XGBoost).
Project 13: Customer Segmentation using K-Means Clustering (K-Means, Hierarchical Clustering, DBSCAN, PCA, T-SNE).
Project 18: Movie Recommendation System (PCA, T-SNE, possibly clustering algorithms).

---

## ğŸ“Œ Getting Started

Start with:
```bash
cd 01_Linear_Regression
python linear_numpy.py
