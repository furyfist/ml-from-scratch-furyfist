#  ML From Scratch – furyfist :)

This repository documents my journey in building core ML algorithms **from scratch** using NumPy, and applying them using libraries like `scikit-learn`. It’s structured for clarity, depth, and real-world understanding.

---

## ✅ Machine Learning Algorithms Tracker

| #  | Algorithm                                             | Theory | NumPy Impl | Sklearn | Project | Notes |
|----|--------------------------------------------------------|--------|------------|---------|---------|-------|
| 1  | Linear Regression                                      | ✅     | ✅         | ✅     | ⬜️      |       |
| 2  | Gradient Descent                                       | ✅     | ✅         | N/A     | ⬜️      | Common optimizer |
| 3  | Logistic Regression                                    | ✅     | ⬜️         | ✅     | ⬜️      | Next after linear |
| 4  | Support Vector Machines (SVM)                          | ⬜️     | ⬜️         | ⬜️      | ⬜️      | Hinge loss, kernel trick |
| 5  | Naive Bayes                                            | ⬜️     | ⬜️         | ⬜️      | ⬜️      | Probabilistic |
| 6  | K-Nearest Neighbors (KNN)                              | ⬜️     | ⬜️         | ⬜️      | ⬜️      | Lazy learning |
| 7  | Decision Trees                                         | ⬜️     | ⬜️         | ⬜️      | ⬜️      | Entropy/Gini |
| 8  | Random Forest                                          | ⬜️     | ⬜️         | ⬜️      | ⬜️      | Ensemble of trees |
| 9  | Bagging                                                | ⬜️     | ⬜️         | ⬜️      | ⬜️      | Bootstrap Aggregation |
| 10 | AdaBoost                                               | ⬜️     | ⬜️         | ⬜️      | ⬜️      | Weighted learners |
| 11 | Gradient Boosting                                      | ⬜️     | ⬜️         | ⬜️      | ⬜️      | Residual-based |
| 12 | XGBoost                                                | ⬜️     | ⬜️         | ⬜️      | ⬜️      | Efficient boosting |
| 13 | Principal Component Analysis (PCA)                     | ⬜️     | ⬜️         | ⬜️      | ⬜️      | Dimensionality reduction |
| 14 | K-Means Clustering                                     | ⬜️     | ⬜️         | ⬜️      | ⬜️      | Unsupervised |
| 15 | Hierarchical Clustering                                | ⬜️     | ⬜️         | ⬜️      | ⬜️      | Dendrograms |
| 16 | DBSCAN                                                 | ⬜️     | ⬜️         | ⬜️      | ⬜️      | Density-based |
| 17 | T-SNE (t-Distributed Stochastic Neighbor Embedding)    | ⬜️     | ⬜️         | ⬜️      | ⬜️      | For high-dim visualization |

---

## 🧭 Learning Workflow

For each algorithm:
1. ✅ Understand the **theory**: intuition, math, derivations.
2. ✅ Implement using **NumPy**: no ML libraries.
3. ✅ Apply with **scikit-learn** (or suitable lib).
4. ✅ Test on small **projects** or datasets.
5. ✅ Document learnings in Markdown.

## Projects to Make : 
Playlist link : https://www.youtube.com/playlist?list=PLfFghEzKVmjvuSA67LszN1dZ-Dd_pkus6

Project 3: House Price Prediction (Linear Regression, Decision Trees, Random Forest, Gradient Boosting, XGBoost).
Project 1: SONAR Rock vs Mine Prediction (Logistic Regression, KNN, Decision Trees, Random Forest, AdaBoost, Gradient Boosting, XGBoost, SVM).
Project 4: Fake News Prediction (Naive Bayes, Logistic Regression, Decision Trees, Random Forest, XGBoost).
Project 13: Customer Segmentation using K-Means Clustering (K-Means, Hierarchical Clustering, DBSCAN, PCA, T-SNE).
Project 18: Movie Recommendation System (PCA, T-SNE, possibly clustering algorithms).

---

## 📌 Getting Started

Start with:
```bash
cd 01_Linear_Regression
python linear_numpy.py
